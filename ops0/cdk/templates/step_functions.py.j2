"""
Step Functions state machine definition for {{ pipeline_name }}
Auto-generated by ops0
"""
from aws_cdk import (
    aws_stepfunctions as sfn,
    aws_stepfunctions_tasks as tasks,
    aws_logs as logs,
    Duration,
    RemovalPolicy
)

def create_state_machine(self, lambdas, bucket):
    """Create Step Functions state machine for the pipeline"""

    # Create retry configuration
    retry_config = sfn.RetryProps(
        max_attempts=3,
        interval=Duration.seconds(2),
        backoff_rate=2.0,
        errors=["States.TaskFailed", "Lambda.ServiceException", "Lambda.AWSLambdaException"]
    )

    # Create catch configuration for error handling
    catch_props = sfn.CatchProps(
        errors=["States.ALL"],
        result_path="$.error"
    )

    {%- for step_name, dependencies in dag.items() %}

    # Task for {{ step_name }}
    {{ step_name }}_task = tasks.LambdaInvoke(
        self, "{{ step_name.replace('_', ' ').title().replace(' ', '') }}Task",
        lambda_function=lambdas["{{ step_name }}"],
        {%- if dependencies %}
        input_path="$",
        {%- endif %}
        output_path="$.Payload",
        result_path=f"$.steps.{{ step_name }}",
        retry_on_service_exceptions=True,
        payload_response_only=False
    )

    # Add retry policy
    {{ step_name }}_task.add_retry(**retry_config._asdict())

    {%- endfor %}

    # Create error handler
    error_handler = sfn.Fail(
        self, "PipelineError",
        cause="Pipeline execution failed",
        error="$.error"
    )

    # Create success state
    success_state = sfn.Succeed(
        self, "PipelineSuccess",
        comment="Pipeline completed successfully"
    )

    # Build the state machine flow
    {%- if execution_order %}
    # Sequential execution based on dependencies
    {% for i, step in enumerate(execution_order) %}
    {%- if i == 0 %}
    definition = {{ step }}_task
    {%- else %}
    definition = definition.next({{ step }}_task)
    {%- endif %}
    {%- endfor %}

    # Add success state at the end
    definition = definition.next(success_state)

    {%- else %}
    # Parallel execution for independent steps
    parallel = sfn.Parallel(self, "ParallelSteps")

    {%- for step_name in dag.keys() %}
    parallel.branch({{ step_name }}_task)
    {%- endfor %}

    definition = parallel.next(success_state)
    {%- endif %}

    # Add global error handling
    definition.add_catch(error_handler, props=catch_props)

    # Create log group for state machine
    log_group = logs.LogGroup(
        self, "StateMachineLogGroup",
        log_group_name=f"/aws/vendedlogs/states/ops0-{self.stage}-{{ pipeline_name }}",
        removal_policy=RemovalPolicy.DESTROY,
        retention=logs.RetentionDays.ONE_WEEK
    )

    # Create the state machine
    state_machine = sfn.StateMachine(
        self, "{{ pipeline_name.replace('_', ' ').title().replace(' ', '') }}StateMachine",
        state_machine_name=f"ops0-{self.stage}-{{ pipeline_name }}",
        definition=definition,
        timeout=Duration.hours({{ timeout_hours }}),
        tracing_enabled=True,
        logs={
            "destination": log_group,
            "level": sfn.LogLevel.ALL,
            "include_execution_data": True
        }
    )

    # Add tags
    state_machine.node.add_metadata("ops0:pipeline", "{{ pipeline_name }}")
    state_machine.node.add_metadata("ops0:version", "{{ version }}")
    state_machine.node.add_metadata("ops0:stage", self.stage)

    # Create CloudWatch dashboard for monitoring
    self._create_dashboard(state_machine, lambdas)

    return state_machine

def _create_dashboard(self, state_machine, lambdas):
    """Create CloudWatch dashboard for pipeline monitoring"""
    from aws_cdk import aws_cloudwatch as cw

    dashboard = cw.Dashboard(
        self, "PipelineDashboard",
        dashboard_name=f"ops0-{self.stage}-{{ pipeline_name }}"
    )

    # State machine metrics
    dashboard.add_widgets(
        cw.GraphWidget(
            title="Pipeline Executions",
            left=[
                state_machine.metric_succeeded(statistic="Sum"),
                state_machine.metric_failed(statistic="Sum"),
                state_machine.metric_throttled(statistic="Sum"),
                state_machine.metric_timed_out(statistic="Sum")
            ],
            width=12,
            height=6
        ),
        cw.GraphWidget(
            title="Pipeline Duration",
            left=[
                state_machine.metric_time(statistic="Average"),
                state_machine.metric_time(statistic="p99")
            ],
            width=12,
            height=6
        )
    )

    # Lambda metrics row
    lambda_widgets = []
    for step_name, lambda_fn in lambdas.items():
        widget = cw.GraphWidget(
            title=f"{step_name} Performance",
            left=[
                lambda_fn.metric_invocations(statistic="Sum"),
                lambda_fn.metric_errors(statistic="Sum")
            ],
            right=[
                lambda_fn.metric_duration(statistic="Average")
            ],
            width=8,
            height=6
        )
        lambda_widgets.append(widget)

    # Add Lambda widgets in rows of 3
    for i in range(0, len(lambda_widgets), 3):
        dashboard.add_widgets(*lambda_widgets[i:i+3])

    # Add custom metrics
    dashboard.add_widgets(
        cw.GraphWidget(
            title="Step Success Rate",
            left=[
                cw.Metric(
                    namespace=f"ops0/{{ pipeline_name }}/{self.stage}",
                    metric_name="StepSuccess",
                    dimensions_map={"Step": step_name},
                    statistic="Sum"
                )
                for step_name in lambdas.keys()
            ],
            width=24,
            height=6
        )
    )

# Additional helper functions for complex workflows

def create_map_state(self, step_name, lambda_function, max_concurrency=10):
    """Create a Map state for parallel processing of arrays"""
    return sfn.Map(
        self, f"{step_name}Map",
        max_concurrency=max_concurrency,
        items_path="$.items",
        parameters={
            "item.$": "$$.Map.Item.Value",
            "index.$": "$$.Map.Item.Index"
        }
    ).iterator(
        tasks.LambdaInvoke(
            self, f"{step_name}MapTask",
            lambda_function=lambda_function,
            payload_response_only=True
        )
    )

def create_choice_state(self, step_name, conditions):
    """Create a Choice state for conditional branching"""
    choice = sfn.Choice(self, f"{step_name}Choice")

    for condition in conditions:
        if condition['type'] == 'numeric_greater_than':
            cho