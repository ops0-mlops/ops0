"""
Lambda function for {{ step_name }} step
Auto-generated by ops0
"""
import os
import json
import sys
import traceback
import cloudpickle
import boto3
from datetime import datetime

# Initialize AWS clients
s3 = boto3.client('s3')
cloudwatch = boto3.client('cloudwatch')

# Environment variables
BUCKET = os.environ['OPS0_BUCKET']
STAGE = os.environ.get('OPS0_STAGE', 'prod')
PIPELINE_NAME = os.environ.get('OPS0_PIPELINE_NAME', '{{ pipeline_name }}')
STEP_NAME = '{{ step_name }}'

# Monitoring namespace
METRICS_NAMESPACE = f'ops0/{PIPELINE_NAME}/{STAGE}'

def send_metric(metric_name, value, unit='Count'):
    """Send custom metric to CloudWatch"""
    try:
        cloudwatch.put_metric_data(
            Namespace=METRICS_NAMESPACE,
            MetricData=[
                {
                    'MetricName': metric_name,
                    'Value': value,
                    'Unit': unit,
                    'Dimensions': [
                        {'Name': 'Step', 'Value': STEP_NAME},
                        {'Name': 'Stage', 'Value': STAGE}
                    ]
                }
            ]
        )
    except Exception as e:
        print(f"Failed to send metric: {e}")

def load_input(event):
    """Load input data from event or S3"""
    # Check if input is directly in event
    if 'input_data' in event:
        return event['input_data']

    # Check if we have S3 references
    if 'input_keys' in event:
        inputs = []
        for key in event['input_keys']:
            response = s3.get_object(Bucket=BUCKET, Key=key)
            data = cloudpickle.loads(response['Body'].read())
            inputs.append(data)
        return inputs if len(inputs) > 1 else inputs[0]

    # Check for single input key
    if 'input_key' in event:
        response = s3.get_object(Bucket=BUCKET, Key=event['input_key'])
        return cloudpickle.loads(response['Body'].read())

    # Return empty dict if no input
    return {}

def save_output(output, execution_id):
    """Save output to S3 and return reference"""
    if output is None:
        return None

    output_key = f"executions/{execution_id}/steps/{STEP_NAME}/output.pkl"

    # Serialize and save to S3
    s3.put_object(
        Bucket=BUCKET,
        Key=output_key,
        Body=cloudpickle.dumps(output),
        Metadata={
            'step': STEP_NAME,
            'pipeline': PIPELINE_NAME,
            'execution_id': execution_id,
            'timestamp': datetime.utcnow().isoformat()
        }
    )

    return output_key

def handler(event, context):
    """AWS Lambda handler"""
    start_time = datetime.utcnow()
    execution_id = event.get('execution_id', context.request_id)

    print(f"Starting step '{STEP_NAME}' for execution {execution_id}")

    try:
        # Load input data
        input_data = load_input(event)

        # Import and execute the step function
        {%- if step_config.source_code %}
        # Embedded step function
        exec('''{{ step_config.source_code }}''')
        step_function = {{ step_config.name }}
        {%- else %}
        # Load step function from package
        from pipeline import {{ step_config.name }} as step_function
        {%- endif %}

        # Execute the step
        {%- if step_config.analysis.arguments %}
        # Unpack inputs based on function signature
        if isinstance(input_data, (list, tuple)):
            result = step_function(*input_data)
        elif isinstance(input_data, dict):
            result = step_function(**input_data)
        else:
            result = step_function(input_data)
        {%- else %}
        # No arguments expected
        result = step_function()
        {%- endif %}

        # Save output
        output_key = save_output(result, execution_id)

        # Calculate execution time
        execution_time = (datetime.utcnow() - start_time).total_seconds()

        # Send metrics
        send_metric('StepExecutions', 1)
        send_metric('StepDuration', execution_time * 1000, 'Milliseconds')
        send_metric('StepSuccess', 1)

        # Return success response
        response = {
            'statusCode': 200,
            'step': STEP_NAME,
            'execution_id': execution_id,
            'duration_seconds': execution_time,
            'output_key': output_key,
            'success': True
        }

        {%- if step_config.analysis.returns %}
        # Include output for next steps
        response['output_data'] = result
        {%- endif %}

        print(f"Step '{STEP_NAME}' completed successfully in {execution_time:.2f}s")
        return response

    except Exception as e:
        # Log error
        error_message = str(e)
        error_traceback = traceback.format_exc()

        print(f"Error in step '{STEP_NAME}': {error_message}")
        print(error_traceback)

        # Send error metrics
        send_metric('StepErrors', 1)
        send_metric('StepFailures', 1)

        # Save error info to S3
        error_key = f"executions/{execution_id}/steps/{STEP_NAME}/error.json"
        s3.put_object(
            Bucket=BUCKET,
            Key=error_key,
            Body=json.dumps({
                'step': STEP_NAME,
                'error': error_message,
                'traceback': error_traceback,
                'timestamp': datetime.utcnow().isoformat(),
                'execution_id': execution_id
            }),
            ContentType='application/json'
        )

        # Return error response
        return {
            'statusCode': 500,
            'step': STEP_NAME,
            'execution_id': execution_id,
            'error': error_message,
            'error_key': error_key,
            'success': False
        }

# Health check endpoint
def health_check():
    """Simple health check for Lambda"""
    return {
        'statusCode': 200,
        'body': json.dumps({
            'status': 'healthy',
            'step': STEP_NAME,
            'stage': STAGE
        })
    }

if __name__ == "__main__":
    # Local testing
    test_event = {
        'input_data': {'test': True},
        'execution_id': 'local-test'
    }
    result = handler(test_event, type('Context', (), {'request_id': 'local'})())
    print(json.dumps(result, indent=2))